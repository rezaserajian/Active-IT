{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rezaserajian/Active-IT/blob/main/CS_263F_Assignment_2_Student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol3ycAtObNad"
      },
      "source": [
        "# CS 263F Assignment 2: Transformer and LLMs for Natural Language Inference (NLI)\n",
        "\n",
        "## Deadline: 11:59 PM, May 14, 2025\n",
        "\n",
        "\n",
        "## Outline\n",
        "- Part 1: Transformer Implementation (55 points)\n",
        "- Part 2: Training and Evaluation via Huggingface Transformer (45 points)\n",
        "\n",
        "## Instructions\n",
        "- Follow the instructions and fill in the code for the sections marked with `# TODO`.\n",
        "- **DO NOT** modify the checking/grading cells. Modifying these cells is strictly prohibited and will be treated as an academic integrity violation which will result in 0 score for this assignment and escalation to The Office of Student Conduct at UCLA.\n",
        "\n",
        "## Submission\n",
        "- **Execution**: Ensure all cells have been run, and outputs are displayed before submission.\n",
        "- **File Naming**: Save your completed notebook with outputs as `hw2.ipynb`.\n",
        "- **Upload**: Submit your `hw2.ipynb` file to Gradescope.\n",
        "\n",
        "Failure to follow these instructions will result in the autograder failing, which will automatically result in 0 points. **No regrading will be done** for submissions with incorrect file names or formats.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DaTBFhBG2m0"
      },
      "source": [
        "## Part 1: Transformer Implementation (55 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 1: Scaled Dot-Product Attention (15 points)\n",
        "\n",
        "In this section, you'll implement the **generalized Scaled Dot-Product Self-Attention** mechanism as taught in the class. This is a core component of the Transformer model. This generalized version can accommodate encoder-only, decoder-only and encoder-decoder Transformer architectures, which means it supports cases where the lengths of the queries (Q) and key-value pairs (K, V) may **differ**. This is crucial for tasks like machine translation, where the encoder and decoder operate over sequences of different lengths.\n",
        "\n",
        "\n",
        "This mechanism takes four inputs: queries (Q), keys (K), values (V), and attention masks. It computes attention weights based on the dot product of Q and K, scaled by the square root of the dimensionality of the keys. This helps stabilize gradients and improve model performance. Once the attention weights are calculated, they are used to combine the values (V) into a weighted sum.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "30zB9u-AnQ_0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8_dkDJJC22Z"
      },
      "outputs": [],
      "source": [
        "# DO NOT alter this cell\n",
        "import torch\n",
        "from torch import nn\n",
        "import math\n",
        "import numpy as np\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABY9ByrbINV0"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "    # Q: FloatTensor of shape (bsz, q_len, d)\n",
        "    # K, V: FloatTensor of shape (bsz, kv_len, d)\n",
        "    # mask: optional, BoolTensor of shape (bsz, q_len, kv_len)\n",
        "    # return: outputs, attention_weights\n",
        "    #   outputs: FloatTensor of shape (bsz, q_len, d), output of attention module\n",
        "    #   attention_weights: FloatTensor of shape (bsz, q_len, kv_len), attention weights between 0-1\n",
        "\n",
        "    # Hint: if mask[i, j, k] is False, that means for the i-th data point in the batch,\n",
        "    #   the attention weight from j-th position in Q to k-th position in KV is zero.\n",
        "    #   To do that, you can set the attention logits to a very small value like -np.inf before feeding to softmax\n",
        "\n",
        "    # TODO:\n",
        "\n",
        "    return outputs, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79QcPjegJ3WJ"
      },
      "source": [
        "You can use the following section to check your implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xYw7TJNIQhV"
      },
      "outputs": [],
      "source": [
        "# DO NOT alter this cell\n",
        "# Verify shape\n",
        "bsz, q_len, kv_len, d = 2, 5, 4, 3\n",
        "q = torch.randn(bsz, q_len, d)\n",
        "k = torch.randn(bsz, kv_len, d)\n",
        "v = torch.randn(bsz, kv_len, d)\n",
        "values, attention = scaled_dot_product_attention(q, k, v)\n",
        "print(f\"the output shape is {values.shape}\")\n",
        "print(f\"the shape of attention weight ({attention.shape}) should be the batch x query x key = {bsz} x {q_len} x {kv_len}\")\n",
        "assert tuple(attention.shape) == (bsz, q_len, kv_len)\n",
        "print(\"Part 1.1.1.a passed\")\n",
        "print()\n",
        "print(\"Checking that attention weights sum to 1 across the key dimension (kv_len).\")\n",
        "assert attention.sum(-1).allclose(torch.ones(bsz, q_len, ))\n",
        "print(\"Part 1.1.1.b passed\")\n",
        "print()\n",
        "print(\"Checking that our implementation produces the same results as PyTorch's built-in function.\")\n",
        "assert values.allclose(F.scaled_dot_product_attention(q, k, v, attn_mask=None))\n",
        "print(\"Part 1.1.1.c passed\")\n",
        "print()\n",
        "# Verify attention mask's function\n",
        "bsz, q_len, kv_len, d = 2, 5, 4, 3\n",
        "q = torch.randn(bsz, q_len, d)\n",
        "k = torch.randn(bsz, kv_len, d)\n",
        "v = torch.randn(bsz, kv_len, d)\n",
        "mask = torch.rand(bsz, q_len, kv_len) > 0.5\n",
        "mask[:, :, 0] = True\n",
        "values, attention = scaled_dot_product_attention(q, k, v, mask=mask)\n",
        "assert tuple(attention.shape) == (bsz, q_len, kv_len)\n",
        "assert attention.sum(-1).allclose(torch.ones(bsz, q_len))\n",
        "print(\"Ensuring that positions excluded by the mask have nearly zero attention values.\")\n",
        "assert (attention[~mask].abs() < 1e-8).all()\n",
        "print(\"Part 1.1.2.a passed\")\n",
        "print()\n",
        "print(\"Verifying that our masked attention output matches PyTorch's built-in function with the mask applied.\")\n",
        "assert values.allclose(F.scaled_dot_product_attention(q, k, v, attn_mask=mask))\n",
        "print(\"Part 1.1.2.b passed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpgNAKEFKHGU"
      },
      "source": [
        "### Section 2: Multi-head Attention (10 points)\n",
        "\n",
        "Multi-head attention allows the model to focus on different parts of the input sequence from multiple perspectives, enhancing the model's ability to capture diverse dependencies. This is achieved by using multiple attention heads, each with its own set of query, key, and value projections. The outputs from all heads are then concatenated together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZPVZSnsKkmx"
      },
      "outputs": [],
      "source": [
        "def multi_head_attention(q, k, v, num_heads: int, mask=None):\n",
        "    # Q: FloatTensor of shape (bsz, q_len, d_model)\n",
        "    # K, V: FloatTensor of shape (bsz, kv_len, d_model)\n",
        "    # mask: optional, BoolTensor of shape (bsz, q_len, kv_len)\n",
        "    # return: outputs, attention_weights\n",
        "    #   outputs: FloatTensor of shape (bsz, q_len, d_model), output of attention module\n",
        "    #   attention_weights: FloatTensor of shape (bsz, num_heads, q_len, kv_len), attention weights between 0-1\n",
        "\n",
        "    # Hint: to perform multi-head attention, you split the `d_model`-dimension features into `num_heads` splits. Each attention head will use d_model // num_heads dimensions\n",
        "    # Hint: For example, if d_model is 12 and num_heads is 3, then the first head uses first 4 features, the second head uses 4-8 features, and the third one use last 4 features\n",
        "    # Hint: Then finally, we concatenate outputs from all heads into the output values\n",
        "    # Hint: You can call the scaled_dot_product_attention function you just implemented for each head's computation\n",
        "    # Hint: In real applications, we also want some linear layers to \"project\" the query, key and values before doing attention, but now we're omitting this step\n",
        "    assert q.shape[-1] % num_heads == 0\n",
        "\n",
        "    # TODO:\n",
        "\n",
        "    return outputs, attention_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsa9ElBWK111"
      },
      "source": [
        "You can use the following section to check your implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHXxyEzYK2WG"
      },
      "outputs": [],
      "source": [
        "# DO NOT alter this cell\n",
        "\n",
        "# Verify shape\n",
        "bsz, q_len, kv_len, d, num_heads = 2, 5, 4, 12, 3\n",
        "q = torch.randn(bsz, q_len, d)\n",
        "k = torch.randn(bsz, kv_len, d)\n",
        "v = torch.randn(bsz, kv_len, d)\n",
        "values, attention = multi_head_attention(q, k, v, num_heads=num_heads)\n",
        "print(f\"the output shape is {values.shape}\")\n",
        "print(f\"the shape of attention weight ({attention.shape}) should be the batch x number of attention heads x query x key(value) = {bsz} x {num_heads} x {q_len} x {kv_len}\")\n",
        "assert tuple(attention.shape) == (bsz, num_heads, q_len, kv_len)\n",
        "print(\"Checking that attention weights sum to 1 across the key dimension (kv_len).\")\n",
        "assert attention.sum(-1).allclose(torch.ones(bsz, num_heads, q_len))\n",
        "print(\"Part 1.2.1.a passed\")\n",
        "print()\n",
        "print(\"Checking that our implementation produces the same results as PyTorch's built-in function.\")\n",
        "standard_values, standard_attention = F.multi_head_attention_forward(\n",
        "    q.permute(1, 0, 2), k.permute(1, 0, 2), v.permute(1, 0, 2),\n",
        "    embed_dim_to_check=d, num_heads=num_heads, attn_mask=None,\n",
        "    use_separate_proj_weight=True, in_proj_weight=None, in_proj_bias=None,\n",
        "    q_proj_weight=torch.eye(d), k_proj_weight=torch.eye(d), v_proj_weight=torch.eye(d),\n",
        "    out_proj_weight=torch.eye(d), out_proj_bias=torch.zeros(d),\n",
        "    add_zero_attn=False, dropout_p=0, is_causal=False,\n",
        "    bias_k=None, bias_v=None, average_attn_weights=False,\n",
        ")\n",
        "assert values.allclose(standard_values.permute(1, 0, 2))\n",
        "assert attention.allclose(standard_attention)\n",
        "print(\"Part 1.2.1.b passed\")\n",
        "print()\n",
        "# Verify attention mask's function\n",
        "bsz, q_len, kv_len, d, num_heads = 2, 5, 4, 12, 3\n",
        "q = torch.randn(bsz, q_len, d)\n",
        "k = torch.randn(bsz, kv_len, d)\n",
        "v = torch.randn(bsz, kv_len, d)\n",
        "mask = torch.rand(bsz, q_len, kv_len) > 0.5\n",
        "mask[:, :, 0] = True\n",
        "values, attention = multi_head_attention(q, k, v, num_heads=num_heads, mask=mask)\n",
        "assert tuple(attention.shape) == (bsz, num_heads, q_len, kv_len)\n",
        "assert attention.sum(-1).allclose(torch.ones(bsz, num_heads, q_len))\n",
        "print(\"Part 1.2.2.a passed\")\n",
        "print()\n",
        "\n",
        "print(\"Ensuring that positions excluded by the mask have nearly zero attention values.\")\n",
        "assert (attention.sum(1)[~mask].abs() < 1e-8).all()\n",
        "standard_values, standard_attention = F.multi_head_attention_forward(\n",
        "    q.permute(1, 0, 2), k.permute(1, 0, 2), v.permute(1, 0, 2),\n",
        "    embed_dim_to_check=d, num_heads=num_heads, attn_mask=(~mask).repeat_interleave(num_heads, dim=0),\n",
        "    use_separate_proj_weight=True, in_proj_weight=None, in_proj_bias=None,\n",
        "    q_proj_weight=torch.eye(d), k_proj_weight=torch.eye(d), v_proj_weight=torch.eye(d),\n",
        "    out_proj_weight=torch.eye(d), out_proj_bias=torch.zeros(d),\n",
        "    add_zero_attn=False, dropout_p=0, is_causal=False,\n",
        "    bias_k=None, bias_v=None, average_attn_weights=False,\n",
        ")\n",
        "print(\"Verifying that our masked attention output matches PyTorch's built-in function with the mask applied.\")\n",
        "assert values.allclose(standard_values.permute(1, 0, 2))\n",
        "assert attention.allclose(standard_attention)\n",
        "print(\"Part 1.2.2.b passed\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMg6QZviLlW9"
      },
      "source": [
        "Below is an overall architecture of transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkronCl8Lly8"
      },
      "outputs": [],
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        # Original Transformer initialization, see PyTorch documentation\n",
        "        nn.init.xavier_uniform_(self.q_proj.weight)\n",
        "        nn.init.xavier_uniform_(self.k_proj.weight)\n",
        "        nn.init.xavier_uniform_(self.v_proj.weight)\n",
        "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
        "        self.q_proj.bias.data.fill_(0)\n",
        "        self.k_proj.bias.data.fill_(0)\n",
        "        self.v_proj.bias.data.fill_(0)\n",
        "        self.o_proj.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        q = self.q_proj(x)  # Note: for actual implementation, we need to do projection\n",
        "        k = self.k_proj(x)\n",
        "        v = self.v_proj(x)\n",
        "        o, _ = multi_head_attention(q, k, v, self.num_heads, mask=mask)\n",
        "        return self.o_proj(o)\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            input_dim - Dimensionality of the input\n",
        "            num_heads - Number of heads to use in the attention block\n",
        "            dim_feedforward - Dimensionality of the hidden layer in the MLP\n",
        "            dropout - Dropout probability to use in the dropout layers\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Attention layer\n",
        "        self.self_attn = MultiheadAttention(input_dim, num_heads)\n",
        "\n",
        "        # Two-layer MLP\n",
        "        self.linear_net = nn.Sequential(\n",
        "            nn.Linear(input_dim, dim_feedforward),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(dim_feedforward, input_dim)\n",
        "        )\n",
        "\n",
        "        # Layers to apply in between the main layers\n",
        "        self.norm1 = nn.LayerNorm(input_dim)\n",
        "        self.norm2 = nn.LayerNorm(input_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Attention part\n",
        "        attn_out = self.self_attn(x, mask=mask)\n",
        "        x = x + self.dropout(attn_out)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        # MLP part\n",
        "        linear_out = self.linear_net(x)\n",
        "        x = x + self.dropout(linear_out)\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, we, pe, layers):\n",
        "        super().__init__()\n",
        "        self.we = we  # word embeddings\n",
        "        self.pe = pe  # positional embeddings\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "        vocab_size, model_dim = we.weight.shape\n",
        "        self.lm_head = nn.Linear(model_dim, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, tokens, mask=None):\n",
        "        x = self.we(tokens) + self.pe(tokens)\n",
        "        for l in self.layers:\n",
        "            x = l(x, mask=mask)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DC3jVajnLouW"
      },
      "source": [
        "### Section 3: Positional Encoding (10 points)\n",
        "\n",
        "Transformers lack an inherent sense of order in sequences, which is why positional encodings are added to the input embeddings. These encodings provide information about the position of tokens in a sequence, allowing the model to differentiate between tokens in different positions.\n",
        "\n",
        "The positional encoding for each element $PE(pos, 2i)$ and $PE(pos, 2i+1)$ is defined as follows:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "    PE(pos, 2i) & = \\sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}), \\\\\n",
        "    PE(pos, 2i+1) & = \\cos(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $pos$ is the position in the sequence.\n",
        "- $i$ is the dimension index (split between even and odd indices).\n",
        "- $d_{model}$ is the dimensionality of the model's input embeddings.\n",
        "\n",
        "This alternating use of sine and cosine allows each position to have a unique encoding across the dimensions, making it easier for the model to distinguish between positions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onDNWExIMO4G"
      },
      "outputs": [],
      "source": [
        "class PositionalEncodings(nn.Module):\n",
        "    def __init__(self, d_model, base=10000):\n",
        "        \"\"\"\n",
        "        Inputs\n",
        "            d_model - Hidden dimensionality of the input.\n",
        "            base - Base for rotary positional encodings.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.base = base\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: FloatTensor of shape (bsz, seq_len)\n",
        "        # return: pe, FloatTensor of shape (bsz, seq_len, d_model)\n",
        "        #   pe[..., i] is the positional encoding for i-th position\n",
        "        bsz, seq_len = x.shape\n",
        "\n",
        "        # TODO:\n",
        "\n",
        "        return pe.to(x.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-yscBdaXKtG"
      },
      "outputs": [],
      "source": [
        "# DO NOT alter this cell\n",
        "d_model = 64\n",
        "\n",
        "pe = PositionalEncodings(d_model).forward(torch.zeros(1, 100)).squeeze(0)\n",
        "print(f\"The shape of positional encoding {pe.shape} should be sequence length x feature dimension = 100 x {d_model}\")\n",
        "assert tuple(pe.shape) == (100, d_model)\n",
        "print(\"Part 1.3.1 passed\")\n",
        "print()\n",
        "print(\"The difference between encodings at different positions should be non-zero, which indicates that each position has a unique encoding\")\n",
        "diff = (pe[:, None, :] - pe[None, :, :]).abs().sum(dim=-1)\n",
        "assert (diff[~torch.eye(100).bool()] != 0).all()\n",
        "print(\"Part 1.3.2 passed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0RfzMSrNwlm"
      },
      "outputs": [],
      "source": [
        "# DO NOT alter this cell\n",
        "\n",
        "n_vocab = 6\n",
        "d_model = 64\n",
        "num_heads = 4\n",
        "n_layers = 4\n",
        "\n",
        "model = TransformerDecoder(\n",
        "    we=nn.Embedding(n_vocab, d_model),\n",
        "    pe=PositionalEncodings(d_model),\n",
        "    layers=[\n",
        "        DecoderBlock(d_model, num_heads, dim_feedforward=2 * d_model)\n",
        "        for _ in range(n_layers)\n",
        "    ],\n",
        ").cuda()\n",
        "print(model)\n",
        "\n",
        "tokens = torch.LongTensor([\n",
        "    [0, 1, 2, 3],\n",
        "    [3, 2, 1, 0],\n",
        "]).cuda()\n",
        "out = model(tokens)\n",
        "print(f\"The output shape is {out.shape}. The expected output shape is batch_size x sequence_length x vocab_size = 2 x 4 x {n_vocab}\")\n",
        "assert tuple(out.shape) == (tokens.shape[0], tokens.shape[1], n_vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pS564L27evsM"
      },
      "source": [
        "### Section 4: Autoregressive Attention Mask (5 points)\n",
        "\n",
        "In this function, you will implement an autoregressive attention mask, which is essential for language modeling tasks where each token can only attend to itself and the tokens that precede it. This prevents the model from \"peeking\" at future tokens, thus preserving the autoregressive property of the model.\n",
        "\n",
        "The autoregressive attention mask (also called as causal attention mask) should be a lower triangular matrix. In other words, the mask is True for positions on or below the diagonal and False for positions above the diagonal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sLLsFaqexbn"
      },
      "outputs": [],
      "source": [
        "def autoregressive_attention_mask(tokens):\n",
        "    # tokens: (bsz, seq_len)\n",
        "    # return: mask, torch.BoolTensor of shape (bsz, seq_len as q_len, seq_len as kv_len)\n",
        "    # Hint: generate an autoregressive attention mask. mask[i, j, k] is True means j-th token can attend to k-th token\n",
        "    #   For autoregressive language modelling task, each position can only attend to itself and its previous tokens, and cannot \"cheat\" and peak future tokens\n",
        "    bsz, seq_len = tokens.shape\n",
        "\n",
        "    # TODO:\n",
        "\n",
        "    return mask.to(tokens.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYbdNgpffRmm"
      },
      "outputs": [],
      "source": [
        "# DO NOT alter this cell\n",
        "\n",
        "mask = autoregressive_attention_mask(torch.zeros(2, 10))\n",
        "print(f\"Checking shape: expected (2, 10, 10), got {mask.shape}\")\n",
        "assert tuple(mask.shape) == (2, 10, 10)\n",
        "print(f\"Checking data type: expected torch.bool, got {mask.dtype}\")\n",
        "assert mask.dtype == torch.bool\n",
        "for i in range(10):\n",
        "    for j in range(10):\n",
        "        if i >= j:\n",
        "            assert mask[:, i, j].all()\n",
        "        else:\n",
        "            assert not mask[:, i, j].any()\n",
        "\n",
        "print(\"Part 1.4 passed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6658EdvQShfK"
      },
      "source": [
        "### Section 5: Language Modeling Objective (15 points)\n",
        "\n",
        "The language modeling loss is commonly used in tasks where the goal is to predict the next word in a sequence. This loss measures the difference between the predicted probabilities of the model and the actual word in the sequence. For this task, cross-entropy loss is often used as it compares the predicted distribution with the true one-hot encoded distribution.\n",
        "Note: Since we are doing next token predictions, **Remember to shift the tokens to get the actual labels**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aUD2OMnSU0H"
      },
      "outputs": [],
      "source": [
        "def language_modelling_loss(tokens, logits):\n",
        "    # tokens: (bsz, seq_len)\n",
        "    # logits: (bsz, seq_len, n_vocab) raw logits predicted from `logits = self.lm_head(x)`\n",
        "    # return: loss, a torch scalar.\n",
        "    # Hint: use https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
        "    #   Since language modelling is essentially a self supervision task, you can create labels from input tokens itself\n",
        "    #   Remember to shift the tokens to get the actual labels\n",
        "\n",
        "    # TODO:\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86GFzvauS_jP"
      },
      "outputs": [],
      "source": [
        "# DO NOT alter this cell\n",
        "\n",
        "loss = language_modelling_loss(tokens, out)\n",
        "print(f\"Checking if the loss is a scalar. Expected shape: (), got {loss.shape}\")\n",
        "\n",
        "assert tuple(loss.shape) == ()\n",
        "print(\"Part 1.5.1 passed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfmMe9D7TKlO"
      },
      "source": [
        "Now let's train our implemented Transformer with a naive task: predicting the next number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYqjSjL8TXWR"
      },
      "outputs": [],
      "source": [
        "# DO NOT alter this cell\n",
        "\n",
        "def train_step(model, optimizer, tokens):\n",
        "    model.zero_grad()\n",
        "    model.train()\n",
        "    mask = autoregressive_attention_mask(tokens)\n",
        "    logits = model(tokens, mask)\n",
        "    loss = language_modelling_loss(tokens, logits)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return float(loss)\n",
        "\n",
        "\n",
        "# Let's use a very naive data to try training: predict the next number\n",
        "tokens = torch.LongTensor([\n",
        "    [0, 1, 2, 3],\n",
        "    [1, 2, 3, 4],\n",
        "    [2, 3, 4, 5],\n",
        "    [5, 4, 3, 2],\n",
        "    [4, 3, 2, 1],\n",
        "    [3, 2, 1, 0],\n",
        "]).cuda()\n",
        "\n",
        "# re-initialize the model, in case the cell is executed multiple times\n",
        "model = TransformerDecoder(\n",
        "    we=nn.Embedding(n_vocab, d_model),\n",
        "    pe=PositionalEncodings(d_model),\n",
        "    layers=[\n",
        "        DecoderBlock(d_model, num_heads, dim_feedforward=2 * d_model)\n",
        "        for _ in range(n_layers)\n",
        "    ],\n",
        ").cuda()\n",
        "\n",
        "# train\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "losses = []\n",
        "for i in range(1000):\n",
        "    loss = train_step(model, optimizer, tokens)\n",
        "    if i % 100 == 0:\n",
        "        print(\"Step {:d}: loss = {:.4f}\".format(i + 1, loss))\n",
        "    losses.append(loss)\n",
        "plt.plot(losses)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tG74SnNoTcwr"
      },
      "outputs": [],
      "source": [
        "# DO NOT alter this cell\n",
        "\n",
        "tokens = torch.LongTensor([\n",
        "    [1, 2, 3],\n",
        "    [3, 2, 1],\n",
        "]).cuda()\n",
        "mask = autoregressive_attention_mask(tokens)\n",
        "logits = model(tokens, mask)\n",
        "prob = nn.Softmax(dim=-1)(logits)\n",
        "\n",
        "print(\"Next token of 1, 2, 3:\")\n",
        "for j in prob[0, -1].argsort(descending=True)[:3]:\n",
        "    print(\"  p({:d}) = {:.4f}\".format(j, prob[0, -1, j]))\n",
        "assert (prob[0, -1].argsort(descending=True)[0] == 4)\n",
        "print(\"Part 1.5.2.a passed\")\n",
        "print()\n",
        "print(\"Next token of 3, 2, 1:\")\n",
        "for j in prob[1, -1].argsort(descending=True)[:3]:\n",
        "    print(\"  p({:d}) = {:.4f}\".format(j, prob[1, -1, j]))\n",
        "assert (prob[1, -1].argsort(descending=True)[0] == 0)\n",
        "print()\n",
        "print(\"Part 1.5.2.b passed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gZz3NPhgqGe"
      },
      "outputs": [],
      "source": [
        "# run this cell before moving into part2 to clear up gpu memory\n",
        "del model\n",
        "import torch\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDMA8eC3VPoq"
      },
      "source": [
        "## Part 2: Training and Evaluation via Huggingface Transformer (45 points)\n",
        "\n",
        "In this part, you will first implement the evaluation of a transformer model on a dataset and then fine-tune the model to observe improvements on task performance.\n",
        "\n",
        "Hints:\n",
        "- You can use a GPU to speed up the training process. Select \"Runtime\" > \"Change runtime type\" > \"GPU\" in the Colab settings.\n",
        "- Use smaller batch sizes if you encounter memory issues.\n",
        "\n",
        "### Prerequisites: Install Libraries and Login to HuggingFace\n",
        "\n",
        "- The model we are going to use is [Llama-3.2-1B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct) which is a gated model. Please visit the model page on Hugging Face and accept the terms of the license. Typically, access is granted within 24 hours.\n",
        "- Go to huggingface -> Profile icon on the upper-right corner -> \"Settings\" -> \"Access Tokens\" to get the key for logging in"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmMtq-DBVs1Q"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate bitsandbytes>0.37.0 trl==0.12.0 peft\n",
        "!pip install flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELvlh-1HVvYt"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOnP7pO6WSre"
      },
      "source": [
        "### Section 1: Loading the model and the tokenizer (15 points)\n",
        "In this section, you will learn how to load the pre-trained model and its associated tokenizer using the huggingface package.\n",
        "\n",
        "You can see a demo code here: <br>\n",
        "https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vd8V6suyWg7l"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# TODO: Load the model using the appropriate parameters using AutoModelForCausalLM\n",
        "# Ensure torch_dtype is set to torch.bfloat16\n",
        "\n",
        "\n",
        "# TODO: Initialize the tokenizer using AutoTokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT alter this cell.\n",
        "vocab_size = len(tokenizer.get_vocab())\n",
        "print(f\"\\nThe vocabulary size of the model is {vocab_size}\")\n",
        "assert vocab_size == 128256\n",
        "print(\"Part 2.1.1 passed\")"
      ],
      "metadata": {
        "id": "-8qwn9rJfal_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Define the messages for the chatbot interaction (List[Dict])\n",
        "messages = [\n",
        "\n",
        "]\n",
        "\n",
        "def run_model(model, tokenizer, messages, max_new_tokens=5, verbose=False):\n",
        "    # TODO: Prepare the input text using the tokenizer's apply_chat_template (Do not tokenize the text yet)\n",
        "\n",
        "    if verbose: print(\"\\n###input_text:###\\n\", input_text)\n",
        "    # TODO: Tokenize the input text and transfer it to the appropriate device\n",
        "\n",
        "    if verbose: print(\"\\n###input_ids:###\\n\", input_ids)\n",
        "    # TODO: Generate a response using the model. Ensure do_sample is False.\n",
        "\n",
        "\n",
        "    # TODO: Decode the output and return the response without special tokens\n",
        "\n",
        "    if verbose: print(\"\\n###response:###\\n\", response)\n",
        "    return assistant_response\n",
        "\n",
        "assistant_response = run_model(model=model, tokenizer=tokenizer, messages=messages, max_new_tokens=10, verbose=True)\n",
        "print(f\"\\n###Assistant response:###\\n{assistant_response}\")"
      ],
      "metadata": {
        "id": "6GghJ121-Brt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHmnM3DmWrUG"
      },
      "source": [
        "Use the following code snippet to verify your implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icc7-5CdWqzM"
      },
      "outputs": [],
      "source": [
        "# DO NOT alter this cell.\n",
        "grading_messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a chatbot who responds very shortly.\"},\n",
        "    {\"role\": \"user\", \"content\": \"When was UCLA founded?\"},\n",
        "]\n",
        "grading_output = run_model(model=model, tokenizer=tokenizer, messages=grading_messages, max_new_tokens=100)\n",
        "expected_output = \"University of California, Los Angeles (UCLA) was founded in 1919.\"\n",
        "print(f\"Your output is: {grading_output}\\nThe expected output is: {expected_output}\")\n",
        "if grading_output != expected_output:\n",
        "    raise ValueError(f\"FAILED: Incorrect response! \\n\\n{grading_output}\\n\\n{expected_output}\")\n",
        "print(\"Part 2.1.2 passed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwBJmIwDW4zB"
      },
      "source": [
        "### Section 2: Evaluation of the pre-trained model (10 points)\n",
        "In this section, we are going to evaluate the pre-trained model with the natural language inference (NLI) task. NLI is a fundamental task in natural language processing that involves determining the relationship between two sentences: a **premise** and a **hypothesis**. The task is to classify this relationship into one of three categories:\n",
        "\n",
        "**Entailment**: The hypothesis logically follows from the premise.\n",
        "\n",
        "**Contradiction**: The hypothesis is logically inconsistent with the premise.\n",
        "\n",
        "**Neutral**: This occurs when there is no clear logical relationship between the premise and the hypothesis.\n",
        "\n",
        "You can view examples for each category by running the cell below.\n",
        "\n",
        "Your task is to implement the evaluation code. This involves running the model on the test set and comparing its predictions with the true labels. The goal is to achieve an accuracy of at least 30% by prompt engineering the model to generate the appropriate labels for this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dk-GGUOlZezy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "# Downloading dataset\n",
        "dataset = {\n",
        "    \"train\": pd.read_json(\"hf://datasets/nlp-projects/cs269-f24/assignment_2/esnli_train.jsonl\", lines=True),\n",
        "    \"validation\": pd.read_json(\"hf://datasets/nlp-projects/cs269-f24/assignment_2/esnli_validation.jsonl\", lines=True),\n",
        "    \"test\": pd.read_json(\"hf://datasets/nlp-projects/cs269-f24/assignment_2/esnli_test.jsonl\", lines=True),\n",
        "}\n",
        "\n",
        "dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ks70OR2wZ-Pq"
      },
      "outputs": [],
      "source": [
        "def apply_esnli_prompt(premise, hypothesis):\n",
        "    # TODO:\n",
        "    # Write a prompt for esnli dataset using given premise and hypothesis\n",
        "    # so that the model classifies the input as entailment, neutral, or contradiction\n",
        "\n",
        "    return prompt.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7oVbn7JaH6w"
      },
      "outputs": [],
      "source": [
        "# Applying your prompt template on the dataset\n",
        "prompt_dataset = {}\n",
        "for part in dataset.keys():\n",
        "    prompt_dataset[part] = dataset[part].copy()\n",
        "    prompt_dataset[part][\"prompt\"] = prompt_dataset[part].apply(lambda x: apply_esnli_prompt(x[\"premise\"], x[\"hypothesis\"]), axis=1)\n",
        "prompt_dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVGDqanEafxI"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def evaluate_esnli(model, tokenizer, test_dataset):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the test dataset.\n",
        "    Returns:\n",
        "        accuracy: The accuracy of the model on the test dataset. The value is scaled from 0.0 to 1.0 (float)\n",
        "        outputs: The model's predictions on the test dataset. (list[str])\n",
        "    \"\"\"\n",
        "    # TODO: Implement the evaluation loop and return accuracy of the model as well as list of outputs\n",
        "    # Hint: You can reuse the run_model function we implemented earlier.\n",
        "\n",
        "\n",
        "    return accuracy, outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkp0iqXKayZu"
      },
      "outputs": [],
      "source": [
        "# DO NOT alter this cell:\n",
        "acc, outputs = evaluate_esnli(model, tokenizer, prompt_dataset[\"test\"])\n",
        "print(f\"Accuracy: {acc}\")\n",
        "prompt_dataset[\"test\"][\"output\"] = outputs\n",
        "expected_acc = 0.3\n",
        "if acc < expected_acc / 2 or acc > 1:\n",
        "  raise ValueError(f\"FAILED: Low Accuracy! \\n\\n{acc} is lower than the required threshold 0.15\\nYou might need to update your prompt so that the model follows the instructions better.\")\n",
        "print(\"Part 2.2.1 passed\")\n",
        "\n",
        "if acc < expected_acc or acc > 1:\n",
        "    raise ValueError(f\"FAILED: Low Accuracy! \\n\\n{acc} is lower than the required threshold {expected_acc}\\nYou might need to update your prompt so that the model follows the instructions better.\")\n",
        "print(\"Part 2.2.2 passed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gcw6bv2IbdB0"
      },
      "source": [
        "### Section 3: Fine-tuning LLaMA (5 points)\n",
        "\n",
        "- (Optional) Read the original LoRA paper [link](https://arxiv.org/pdf/2106.09685) and understand the meaning of each parameters\n",
        "- Complete the LoRA config in the code and fine-tune the model.\n",
        "- The goal is to achieve an accuracy of at least 50%. You can change your prompt in the previous section if your fine-tuned model doesn't achieve this threshold.\n",
        "\n",
        "#### Hint\n",
        "- If you encountered the CUDA out-of-memory (OOM) issue, go to `Runtime` -> `Restart runtime...` in the menu\n",
        "- On T4 GPU the training would take around 15 minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVeNljWiljLX"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "def create_lora_config():\n",
        "    peft_config = LoraConfig(\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=[\n",
        "            \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\",\n",
        "        ],\n",
        "        # TODO: Set r=32 and alpha=16\n",
        "\n",
        "    )\n",
        "    return peft_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAaX2KP8cd7_"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import torch\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
        "from transformers import TrainingArguments, HfArgumentParser, AutoTokenizer, TrainerCallback\n",
        "from huggingface_hub import login\n",
        "import matplotlib.pyplot as plt\n",
        "from peft import LoraConfig\n",
        "\n",
        "# Assigning labels as 'completion' for the prompt dataset.\n",
        "for key in prompt_dataset:\n",
        "    prompt_dataset[key][\"completion\"] = prompt_dataset[key][\"label\"]\n",
        "\n",
        "# Defining the training arguments. These control various aspects of training such as learning rate,\n",
        "# batch size, number of epochs, evaluation strategy, etc.\n",
        "training_args = TrainingArguments(\n",
        "    report_to=\"none\",\n",
        "    learning_rate=5e-5,\n",
        "    lr_scheduler_type=\"constant_with_warmup\",\n",
        "    warmup_steps=10,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=1,\n",
        "    output_dir=\"output_model\",\n",
        "    overwrite_output_dir=True,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=1,\n",
        "    load_best_model_at_end=False,\n",
        "    logging_steps=1,\n",
        "    seed=0,\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    do_predict=False,\n",
        "    eval_strategy=\"epoch\",\n",
        "    gradient_checkpointing=True,\n",
        "    max_grad_norm=0.3,\n",
        "    push_to_hub=False,\n",
        "    hub_private_repo=True,\n",
        ")\n",
        "\n",
        "# Create a LoRA (Low-Rank Adaptation) configuration for parameter-efficient fine-tuning.\n",
        "# This reduces the number of parameters to train and makes the model lighter and faster.\n",
        "peft_config = create_lora_config()\n",
        "\n",
        "# This function formats the input dataset according to a specific template\n",
        "# expected by the model, turning prompts and completions into a chat-based format.\n",
        "def instructions_formatting_function(tokenizer: AutoTokenizer):\n",
        "    def format_dataset(examples):\n",
        "        if isinstance(examples[\"prompt\"], list):\n",
        "            output_texts = []\n",
        "            for i in range(len(examples[\"prompt\"])):\n",
        "                converted_sample = [\n",
        "                    {\"role\": \"user\", \"content\": examples[\"prompt\"][i]},\n",
        "                    {\"role\": \"assistant\", \"content\": examples[\"completion\"][i]},\n",
        "                ]\n",
        "                output_texts.append(tokenizer.apply_chat_template(converted_sample, tokenize=False))\n",
        "            output_texts = [text.replace(\"<s>\", \"\").replace(\"<|begin_of_text|>\", \"\").replace(\"\\n\\n\", \"\") for text in output_texts]\n",
        "            print(output_texts[0])\n",
        "            return output_texts\n",
        "        else:\n",
        "            converted_sample = [\n",
        "                {\"role\": \"user\", \"content\": examples[\"prompt\"]},\n",
        "                {\"role\": \"assistant\", \"content\": examples[\"completion\"]},\n",
        "            ]\n",
        "            return tokenizer.apply_chat_template(converted_sample, tokenize=False)\n",
        "\n",
        "    return format_dataset\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "if getattr(tokenizer, \"pad_token\", None) is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Initialize the data collator, which is responsible for completion tasks.\n",
        "# It ensures that all the tokens of the labels are set to an 'ignore_index'\n",
        "# when they do not come from the assistant. This ensure that the loss is only\n",
        "# calculated on the completion made by the assistant.\n",
        "response_template = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
        "print(\"response_template:\", response_template)\n",
        "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
        "\n",
        "# Initialize the trainer with all the specified configurations, datasets, and formatting functions.\n",
        "trainer = SFTTrainer(\n",
        "    model_id,\n",
        "    args=training_args,\n",
        "    train_dataset=datasets.Dataset.from_pandas(prompt_dataset[\"train\"]),\n",
        "    eval_dataset=datasets.Dataset.from_pandas(prompt_dataset[\"validation\"]),\n",
        "    packing=False,\n",
        "    model_init_kwargs={\n",
        "        \"torch_dtype\": torch.bfloat16,\n",
        "    },\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_length=500,\n",
        "    peft_config=peft_config,\n",
        "    formatting_func=instructions_formatting_function(tokenizer),\n",
        "    data_collator=collator,\n",
        ")\n",
        "# Debugging: Print a batch of decoded input tokens from the training dataset to verify they are correct.\n",
        "print(trainer.tokenizer.batch_decode(trainer.train_dataset[\"input_ids\"][0], skip_special_tokens=False))\n",
        "# Run evaluation on the model to check performance on the validation dataset before finetuning.\n",
        "print(trainer.evaluate())\n",
        "# Begin the training loop. This will train the model using the defined parameters and dataset.\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUAIKGy_dIP1"
      },
      "outputs": [],
      "source": [
        "# DO NOT alter this cell:\n",
        "acc, outputs = evaluate_esnli(trainer.model, tokenizer, prompt_dataset[\"test\"])\n",
        "print(f\"Accuracy: {acc}\")\n",
        "prompt_dataset[\"test\"][\"output\"] = outputs\n",
        "expected_acc = 0.5\n",
        "if acc < expected_acc:\n",
        "    raise ValueError(f\"FAILED: Low Accuracy! \\n\\n{acc} is lower than the required threshold {expected_acc}\\nYou might need to update your prompt so that the model follows the instructions better.\")\n",
        "print(\"Part 2.3 passed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETw_Y7kXQurU"
      },
      "source": [
        "### Section 4: Explanation (15 points)\n",
        "\n",
        "\n",
        "* In this section, you'll use the e-SNLI dataset, which includes free-form rationales (explanations) for each NLI example. The task is to generate model explanations for a subset of the test set and compare them to human explanations using the BLEU score. Your primary task is to write effective prompts that will guide the model to generate explanations for a subset of the test set.\n",
        "\n",
        "* Expectations:\n",
        "    * Pretrained Model: Expected BLEU score > 0.15.\n",
        "    * Fine-Tuned Model: Expected BLEU score > 0.20.\n",
        "\n",
        "* To improve the model's explanations, you'll need to engage in prompt engineering. Adjust your input prompts to guide the model toward better explanations (More similar to human references) and aim to improve the BLEU score. You are also encouraged to check the generated explanations to qualitatively understand their quality, and possibly conduct some analysis.\n",
        "\n",
        "* By the end of this section, you should be able to create effective prompts that improve the quality of the model's explanations and analyze their performance through BLEU scores, and optionally do a qualitative analysis by reviewing the explanations and comparing them with human-provided ones.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnxOA-hlQpBV"
      },
      "outputs": [],
      "source": [
        "explanation_dataset = pd.read_json(\"hf://datasets/nlp-projects/cs269-f24/assignment_2/esnli_test_with_explanation.jsonl\", lines=True)\n",
        "explanation_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5UzvkotQ8y3"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def apply_esnli_prompt_explanation(premise, hypothesis):\n",
        "    # TODO:\n",
        "    # Create a prompt for the e-SNLI dataset.\n",
        "    # This prompt will guide the model to generate an explanation for the relationship\n",
        "    # between the given premise and hypothesis.\n",
        "\n",
        "    return prompt.strip()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE\n",
        "def evaluate_esnli_explanation(model, tokenizer, test_dataset, max_new_tokens=1000):\n",
        "    outputs = []\n",
        "    for row in tqdm(test_dataset.to_dict(orient=\"records\")):\n",
        "        # Construct the messages to send to the model.\n",
        "        grading_messages = [\n",
        "            {\"role\": \"system\", \"content\": \"\"},\n",
        "            {\"role\": \"user\", \"content\": row[\"prompt\"]},\n",
        "        ]\n",
        "        # Run the model with the constructed messages and store the output.\n",
        "        output = run_model(model=model, tokenizer=tokenizer, messages=grading_messages, max_new_tokens=max_new_tokens)\n",
        "        # print(output)\n",
        "        outputs.append(output)\n",
        "    # Initialize lists to hold references and hypotheses for BLEU score calculation.\n",
        "    r, h = [], []\n",
        "    for idx, row in tqdm(enumerate(test_dataset.to_dict(orient=\"records\"))):\n",
        "        # Get the human-provided explanations (references) from the dataset.\n",
        "        references = [\n",
        "            row[\"explanation_1\"].split(),  # Split into list of words\n",
        "            row[\"explanation_2\"].split(),\n",
        "            row[\"explanation_3\"].split(),\n",
        "        ]\n",
        "        # Split the generated output (hypothesis) into words.\n",
        "        hypothesis = outputs[idx].split()\n",
        "        r.append(references)\n",
        "        h.append(hypothesis)\n",
        "\n",
        "    # Calculate the BLEU score using the nltk library.\n",
        "    # weights=(1, 0, 0, 0) means we are using only the 1-gram BLEU score.\n",
        "    bleu_score = nltk.translate.bleu_score.corpus_bleu(r, h, weights=(1, 0, 0, 0))\n",
        "    return bleu_score, outputs  # Return the calculated BLEU score and the generated outputs."
      ],
      "metadata": {
        "id": "r8ti61DDbjyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nm-1T765RLbY"
      },
      "outputs": [],
      "source": [
        "# DO NOT CHANGE (Testing Pre-trained Model)\n",
        "prompt_explanation_dataset = {}\n",
        "prompt_explanation_dataset[\"test\"] = explanation_dataset.copy()\n",
        "prompt_explanation_dataset[\"test\"][\"prompt\"] = prompt_explanation_dataset[\"test\"].apply(lambda x: apply_esnli_prompt_explanation(x[\"premise\"], x[\"hypothesis\"]), axis=1)\n",
        "prompt_explanation_dataset[\"test\"]\n",
        "\n",
        "df = prompt_explanation_dataset[\"test\"].iloc[:10].copy()\n",
        "bleu_score, outputs = evaluate_esnli_explanation(model, tokenizer, df)\n",
        "print(f\"Bleu: {bleu_score}\")\n",
        "df[\"output\"] = outputs\n",
        "display(df)\n",
        "expected_bleu = 0.15\n",
        "if bleu_score < expected_bleu:\n",
        "    raise ValueError(f\"FAILED: Low Bleu! \\n\\n{bleu_score} is lower than the required threshold {expected_bleu}\\nYou might need to update your prompt so that the model explains more similarly to human explanations.\")\n",
        "print(\"Part 2.4.1 passed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTOTgvVZRTFs"
      },
      "outputs": [],
      "source": [
        "# DO NOT CHANGE (Testing Fine-tuned Model)\n",
        "prompt_explanation_dataset = {}\n",
        "prompt_explanation_dataset[\"test\"] = explanation_dataset.copy()\n",
        "prompt_explanation_dataset[\"test\"][\"prompt\"] = prompt_explanation_dataset[\"test\"].apply(lambda x: apply_esnli_prompt_explanation(x[\"premise\"], x[\"hypothesis\"]), axis=1)\n",
        "prompt_explanation_dataset[\"test\"]\n",
        "\n",
        "df = prompt_explanation_dataset[\"test\"].iloc[:10].copy()\n",
        "bleu_score, outputs = evaluate_esnli_explanation(trainer.model, tokenizer, df)\n",
        "print(f\"Bleu: {bleu_score}\")\n",
        "df[\"output\"] = outputs\n",
        "display(df)\n",
        "expected_bleu = 0.20\n",
        "if bleu_score < expected_bleu:\n",
        "    raise ValueError(f\"FAILED: Low Bleu! \\n\\n{bleu_score} is lower than the required threshold {expected_bleu}\\nYou might need to refine your prompt to encourage the model to generate explanations more similar to human-provided explanations. Since the fine-tuned model is trained primarily to output labels (entailment, contradiction, neutral), it's important to create a more assertive prompt that explicitly directs the model to provide an explanation rather than just a label.\")\n",
        "print(\"Part 2.4.2 passed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Dz6vJ1DToYD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}